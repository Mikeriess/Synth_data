{
    "base_model": "google/gemma-2b-it",
    "base_model_config": "google/gemma-2b-it",
    "model_type": "GemmaForCausalLM",
    "tokenizer_type": "GemmaTokenizer",
    "is_llama_derived_model": false,

    "load_in_4bit": true,
    "quantization_config": {
        "load_in_4bit": true,
        "bnb_4bit_use_double_quant": true,
        "bnb_4bit_quant_type": "nf4",
        "bnb_4bit_compute_dtype": "bfloat16"
    },
    "use_gradient_checkpointing": true,
    
    "datasets": [
        {
            "path": "data/chatml_conversations",
            "type": "formatted",
            "conversation_template": "chatml",
            "train_on_inputs": false,
            "sequence_len": 4096,
            "format": "messages"
        }
    ],

    "sequence_len": 4096,
    "sample_packing": false,
    "pad_to_sequence_len": false,

    "lora_r": 8,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "lora_target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],

    "micro_batch_size": 1,
    "gradient_accumulation_steps": 16,
    "num_epochs": 3,
    "learning_rate": 5e-5,
    "train_on_inputs": false,
    "group_by_length": false,
    "bf16": true,
    "fp16": false,
    "tf32": true,

    "warmup_ratio": 0.1,
    "save_steps": 50,
    "eval_steps": 50,
    "logging_steps": 10,
    "output_dir": "outputs/gemma-2b-finetune",

    "flash_attention": true,
    "device_map": "auto"
} 